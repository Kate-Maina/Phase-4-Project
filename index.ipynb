{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194754ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\user\\anaconda3\\moringa\\lib\\site-packages (3.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\moringa\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\moringa\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "All libraries imported successfully!\n",
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import html\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "%pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Environment setup complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2006e",
   "metadata": {},
   "source": [
    "\n",
    "### Business Understanding\n",
    "A real world problem for any business is finding out the general sentiment about one's product. The data can obviously be found in twitter but a challenge is gathering the data and finding the average sentiment about a product. If one was to perform this task manually they can spend a large amount of time to discover what the general public thinks about their product. For a business this can be fatal since if a product has bad general sentiments the business will lose money since customers will stop buying the product and might influence others on not buying the product nor using the service. It is important for businesses to quickly know what the public thinks of their product, so that any needed changes can be made to salvage both the reputation of a business and to maximize the profits.\n",
    "\n",
    "One way to solve this issue is by using a prediction model that can tell one if the sentiments about a product are positive or negative.The dataset we are using is a twitter dataset that was gathered by Crowdflower.\n",
    "\n",
    "The first column is the text of the tweet, the second column denotes whether they are talking about google or apple and the third column states whether the sentiment is positive or negative.The main goal of the project is to predict the sentiment of a tweet from raw text.\n",
    "\n",
    "The value of doing so is essential to any business because one can pull information from social media and get an overview of the sentiments about a certain product. This helps a company evaluate which of their products are the most popular. This allows the company decide which products they should invest more in. It can also tell a company which products are viewed unfavorably, so that the company can change the product. If a company is doing some A/B testing of a product through timed releases the sentiments online can be used to tell which version of a product is the best.\n",
    "\n",
    "The business advantage of having reliable sentiment predictions is great. Shareholders as well as the workers in a company greatly benefit because of the reasons mentioned above.\n",
    "\n",
    "The requirements for creating a model that can predict sentiments from tweets are:\n",
    "        1.Raw Text from social media.\n",
    "        2.Computational power to train the model.\n",
    "        3.A team of data scientists to design the model.\n",
    "        4.A well Labeled validation data to test the efficacy of a model.\n",
    "'\\'\n",
    "The challenges of this project are:\n",
    "        1.Gathering of textual data.\n",
    "        2.Cleaning the raw text so that it is readable by a model.\n",
    "\n",
    "The solution to the problems above:\n",
    "        1.The CrowdFlower dataset gives one enough text scraped from twitter for one to perform the project.\n",
    "        2.The NLTK library and regex python libraries provide one with the tools to clean textual data.\n",
    "        3.The Sklearn python library will provide the models that will be trained on the cleaned textual data\n",
    "\n",
    "\n",
    "###  Data Understanding\n",
    "\n",
    "Before building any models, we took time to understand the structure and contents of the dataset. This step was essential to identify what kind of information we were working with and how to prepare it effectively.\n",
    "\n",
    "1. Dataset Overview The dataset contains tweets that mention various brands or products. Along with each tweet, there's a label indicating the emotional response expressed toward the brand or product. These labels are useful for sentiment or emotion classification.\n",
    "\n",
    "2. Main Columns The most important columns in the dataset are: tweet_text: The raw tweet content, which we use as the input feature,\n",
    "Is there an emotion directed at a brand or product, The target label, which shows the type of emotional reaction (if any) present in the tweet.\n",
    "\n",
    "3. Label Distribution We examined the unique classes in the target column to understand the types and balance of emotional responses. The Classes include:\n",
    "-Positive emotion\n",
    "\n",
    "-Negative emotion\n",
    "\n",
    "-No emotion toward brand or product.\n",
    "\n",
    "4.Missing Values We found some tweets with missing content. These rows were dropped during data preparation to ensure clean input.\n",
    "\n",
    "5.Text Characteristics, we observed the following traits in the tweet text: Informal and noisy (containing emojis, hashtags, mentions, and URLs),\n",
    "Varying lengths, from very short comments to longer sentences, Use of slang, repeated characters and abbreviations. This confirmed that proper text preprocessing (like cleaning, tokenizing, and lemmatization) would be necessary to improve model accuracy.\n",
    "\n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "To get our dataset ready for modeling, we followed a structured set of data cleaning and preprocessing steps. Below is a summary of everything we did:\n",
    "\n",
    "1. Data Loading. We began by loading the dataset, which contains tweets about products and companies. This gave us access to the raw text data along with labels indicating whether there was an emotion directed at a brand or product.\n",
    "\n",
    "2.Initial Inspection. We examined the dataset to understand its structure. This included checking the number of rows and columns, identifying any missing values, and getting a general sense of the data types. We also looked at the unique values in the target column to understand the kinds of emotional labels we would be working with.\n",
    "\n",
    "3.Stopword Setup. Before processing the text, we prepared a list of English stopwords. These are common words like “the”, “and”, “is”, which usually don’t add much value to analysis and can be removed.\n",
    "\n",
    "4.Text Cleaning. Since our dataset was based on tweets, the text was informal and often messy. To make it usable for modeling, we applied several cleaning steps: \n",
    "-Emoji Removal: Emojis were removed to reduce noise.\n",
    "\n",
    "-Hashtag and Mention Removal: We stripped out hashtags and user mentions (like @username) which were not relevant for our analysis.\n",
    "\n",
    "-Link Removal: Any links or URLs in the tweets were removed.\n",
    "\n",
    "-Special Character Filtering: We removed characters and symbols that were not letters, like punctuation or numbers.\n",
    "\n",
    "-Whitespace Handling: Extra spaces were cleaned up to make the text consistent.\n",
    "\n",
    "-Repeated Letters: Words with exaggerated letters (like “wooooow” or “heeeloooooo”) were cleaned by reducing excessive repetition.\n",
    "\n",
    "5.Tokenization and Stopword Removal. We broke each tweet into individual words — a process called tokenization — and removed the stopwords we had prepared earlier. This helped us focus only on the most meaningful words in each tweet.\n",
    "\n",
    "6.Testing and Validation to make sure our logic was working as expected, we tested some of the cleaning and filtering steps with simple examples. This helped us confirm that the functions we wrote were correctly removing or modifying the right parts of the text.\n",
    "\n",
    "7.Pipeline Integration. We used  these steps into a pipeline using Scikit-learn’s tools. This made the process repeatable and easier to manage. The pipeline also included vectorization (using TF-IDF) and later integrated with various machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d1822e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data into a dataframe\n",
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv',encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca3db8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9eb507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9092</td>\n",
       "      <td>3291</td>\n",
       "      <td>9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9065</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>946</td>\n",
       "      <td>5389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet_text  \\\n",
       "count                                                9092   \n",
       "unique                                               9065   \n",
       "top     RT @mention Marissa Mayer: Google Will Connect...   \n",
       "freq                                                    5   \n",
       "\n",
       "       emotion_in_tweet_is_directed_at  \\\n",
       "count                             3291   \n",
       "unique                               9   \n",
       "top                               iPad   \n",
       "freq                               946   \n",
       "\n",
       "       is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "count                                                9093  \n",
       "unique                                                  4  \n",
       "top                    No emotion toward brand or product  \n",
       "freq                                                 5389  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eebc6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_text                                             0.010997\n",
      "emotion_in_tweet_is_directed_at                       63.807324\n",
      "is_there_an_emotion_directed_at_a_brand_or_product     0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((df.isnull().sum()/len(df))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e04f41",
   "metadata": {},
   "source": [
    "- Replaced null on the 'emotion_in_tweet_is_directed_at' column with 'Unknown' to keep more data for training and testing since it contains 64% of the dataset  \n",
    "- Dropped for the remaining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d0d23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion_in_tweet_is_directed_at'] = df['emotion_in_tweet_is_directed_at'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53677f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values for the remaining null values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db35fd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at the classes in the target column\n",
    "df['is_there_an_emotion_directed_at_a_brand_or_product'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d94db",
   "metadata": {},
   "source": [
    "Useful Columns in determining the sentiment of tweets include;\n",
    "\n",
    "-Tweet_text\n",
    "is_there_an_emotion_directed_at_a_brand_or_product\n",
    "-The middle column which is called 'emotion_in_tweet_is_directed_at' is not useful for our aims. It will not be used in the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d43cea",
   "metadata": {},
   "source": [
    "We created a class named TweetPreprocessor and it will have functions for cleaning the text,Transforming the dataframe and creating a fit for training.\n",
    "\n",
    "We will use this class within a pipeline. We will have our class inherit from Sklearn's BaseEstimator and TransformerMixin libraries.\n",
    "\n",
    "BaseEstimator and TransformerMixin make a custom class:\n",
    "\n",
    "1.Scikit-learn compliant.\n",
    "2.Fully usable in Pipeline, GridSearchCV and cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd6036cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, \n",
    "                 remove_stopwords=True, \n",
    "                 min_token_len=3, #removing tokens with less than a length of 3\n",
    "                 lemmatize_words=True,\n",
    "                 output_format='tokens'):  # 'tokens' or 'string'\n",
    "        \n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.min_token_len = min_token_len\n",
    "        self.lemmatize_words = lemmatize_words\n",
    "        self.output_format = output_format\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def _is_english(self, text):\n",
    "        try:\n",
    "            return detect(text) == 'en'\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        #to make sure that the text is in string format\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) if text is not None else ''\n",
    "\n",
    "        # --- Preprocessing Steps ---\n",
    "        text = emoji.replace_emoji(text, replace='')                     # Remove emojis\n",
    "        text = text.lower()                                              # Lowercase\n",
    "        text = re.sub(r'^rt\\s+', '', text)                               # Remove 'RT' tag\n",
    "        text = re.sub(r'https?://\\S+', '', text)                         # Remove URLs\n",
    "        text = re.sub(r'[@#\\$]\\w+', '', text)                            # Remove mentions, hashtags, cashtags\n",
    "        text = re.sub(r'\\d+', '', text)                                  # Remove numbers\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1', text)                         # Reduce character repetition\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)                             # Keep only letters and spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()                         # Normalize whitespace\n",
    "\n",
    "        # --- Tokenization ---\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # --- Filtering ---\n",
    "        #checking and removing stop words\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        #checking for and removing tokens which are less than the threshold specified in init.\n",
    "        if self.min_token_len > 0:\n",
    "            tokens = [word for word in tokens if len(word) >= self.min_token_len]\n",
    "        #lemmatizing\n",
    "        if self.lemmatize_words:\n",
    "            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        # --- Return format ---\n",
    "        #combining the tokens with spaces in the middle to create strings if we specify string in the output_format\n",
    "        return ' '.join(tokens) if self.output_format == 'string' else tokens\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Accept both list and Series (important for LIME compatibility)\n",
    "        if isinstance(X, (list, pd.Series)):\n",
    "            return [self.clean_text(text) for text in X]\n",
    "        raise TypeError(f\"Expected list or pd.Series, got {type(X)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
